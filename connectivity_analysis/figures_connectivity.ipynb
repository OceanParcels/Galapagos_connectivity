{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors, cm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gstools as gs\n",
    "from collections import OrderedDict\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "import cartopy.crs as ccrs\n",
    "from cartopy.io.img_tiles import OSM\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.io import shapereader\n",
    "from cartopy.io.img_tiles import Stamen\n",
    "from cartopy.io.img_tiles import GoogleTiles\n",
    "from owslib.wmts import WebMapTileService\n",
    "from matplotlib.transforms import offset_copy\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "import networkx as nx\n",
    "import json\n",
    "\n",
    "parent_dir = str(Path(os.path.dirname(os.path.realpath(\"__file__\"))).parents[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "# Load all transition matrices\n",
    "transitionmatrix = np.load('output/Tm_tbeach5.npy')\n",
    "TM1 = np.load('output/Tm_tbeach1.npy')\n",
    "TM2 = np.load('output/Tm_tbeach2.npy')\n",
    "TM10 = np.load('output/Tm_tbeach10.npy')\n",
    "TM26 = np.load('output/Tm_tbeach26.npy')\n",
    "TM35 = np.load('output/Tm_tbeach35.npy')\n",
    "TM100 = np.load('output/Tm_tbeach100.npy')\n",
    "TM_all = np.load('output/Tm_all_particles.npy')\n",
    "TM_half = np.load('output/Tm_half_particles.npy')\n",
    "\n",
    "# load grid specifics\n",
    "gridsdataframe = pd.read_csv(parent_dir + '/particle_simulation/input/GridsDataFrame.csv')\n",
    "gridnumbermask = np.load(parent_dir + '/particle_simulation/input/gridnumbermask.npy')\n",
    "gridlon = np.load(parent_dir + '/particle_simulation/input/x_corners_grid.npy')\n",
    "gridlat = np.load(parent_dir + '/particle_simulation/input/y_corners_grid.npy')\n",
    "lat_release = np.load(parent_dir + '/particle_simulation/input/ReleaseLat.npy')\n",
    "lon_release = np.load(parent_dir + '/particle_simulation/input/ReleaseLon.npy')\n",
    "endloc = np.load('output/endloc.npy')\n",
    "\n",
    "# number of coastal grids\n",
    "c_total = len(gridsdataframe) \n",
    "\n",
    "# number of particles released at each coastgrid (daily release)\n",
    "p_total = np.array(endloc.shape[1])/c_total\n",
    "\n",
    "# Details needed to seperate islands\n",
    "channel_gridID = np.array([331, 364, 363, 362, 361, 360])-1\n",
    "fernandina_gridID = np.arange(332,360)-1\n",
    "\n",
    "# Location of ports Galapagos Islands (Fig. 1)\n",
    "lat_ports = [-0.95479, -1.27542, -0.90128, -0.74633, -0.46186]\n",
    "lon_ports = [-90.96502, -90.48985, -89.61261, -90.30874, -90.27337]\n",
    "\n",
    "# load example trajectories (Fig. 1)\n",
    "lons = np.load('input/traj_lon107.npy')\n",
    "lats = np.load('input/traj_lat107.npy')\n",
    "lons[lons==0]=np.nan\n",
    "lats[lats==0]=np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing: reorder coastal grids\n",
    "\n",
    "Coastal cells need to be ordered by island number, labeled in a clockwise direction starting at the northernmost point and starting with the most eastern islands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Get all coastal cells in the right order\n",
    "\n",
    "# extend gridsdataframe to include index and lat/lon of grid center\n",
    "coastal_cells=np.where(gridnumbermask>0)\n",
    "gridsdataframe['index_lat'] = coastal_cells[0]\n",
    "gridsdataframe['index_lon'] = coastal_cells[1]\n",
    "gridsdataframe['lat'] = lat_release\n",
    "gridsdataframe['lon'] = lon_release\n",
    "\n",
    "# Add a new column that will specify the new order of coastal cells\n",
    "gridsdataframe['index_coastline'] = 0\n",
    "\n",
    "# start variables\n",
    "islands = np.array([6,3,5,4,2,9,7,8,1]) #correct order\n",
    "index_teller = 0\n",
    "\n",
    "# Loop through all islands\n",
    "\n",
    "for nisland in islands:\n",
    "    index_teller += 1\n",
    "    grid_island = gridsdataframe[gridsdataframe.island_number==nisland]\n",
    "\n",
    "    # Get the starting point \n",
    "    index_start = grid_island.index_lat.argmax() #first most northern grid point\n",
    "    index_start_table = grid_island.iloc[index_start,:].name # row index of this point in table\n",
    "    gridsdataframe.loc[index_start_table,'index_coastline']=index_teller # set to first location\n",
    "    index_teller += 1\n",
    "\n",
    "    # Get second point (closest in clockwise direction)\n",
    "    start_lat = grid_island.loc[index_start_table,'index_lat']\n",
    "    start_lon = grid_island.loc[index_start_table,'index_lon']\n",
    "    if gridnumbermask[start_lat,start_lon+1]>0: # to the east\n",
    "        outcome=np.logical_and(gridsdataframe.index_lat==start_lat,gridsdataframe.index_lon==start_lon+1)\n",
    "        gridsdataframe.loc[outcome,'index_coastline']=index_teller\n",
    "        new_lat = gridsdataframe.loc[outcome,'index_lat']\n",
    "        new_lon = gridsdataframe.loc[outcome,'index_lon']\n",
    "    elif gridnumbermask[start_lat-1,start_lon]>0: # to the south\n",
    "        outcome=np.logical_and(gridsdataframe.index_lat==start_lat-1,gridsdataframe.index_lon==start_lon)\n",
    "        gridsdataframe.loc[outcome,'index_coastline']=index_teller\n",
    "        new_lat = gridsdataframe.loc[outcome,'index_lat']\n",
    "        new_lon = gridsdataframe.loc[outcome,'index_lon']\n",
    "    elif gridnumbermask[start_lat,start_lon-1]>0: # to the west\n",
    "        outcome=np.logical_and(gridsdataframe.index_lat==start_lat,gridsdataframe.index_lon==start_lon-1)\n",
    "        gridsdataframe.loc[outcome,'index_coastline']=index_teller\n",
    "        new_lat = gridsdataframe.loc[outcome,'index_lat']\n",
    "        new_lon = gridsdataframe.loc[outcome,'index_lon']\n",
    "    else:\n",
    "        print('something goes wrong with assigning second coastal cell')  \n",
    "\n",
    "    # Loop through remaining points and assign each closest point that is not already assigned\n",
    "\n",
    "    for i in range(len(grid_island)-2):\n",
    "        grid_island = gridsdataframe[gridsdataframe.island_number==nisland]\n",
    "        index_teller += 1\n",
    "        distance = ((grid_island[grid_island.index_coastline==0].index_lat - np.array(new_lat))**2 + \n",
    "                    (grid_island[grid_island.index_coastline==0].index_lon - np.array(new_lon))**2)\n",
    "        dist_min = distance.argmin()\n",
    "        index_next = grid_island[grid_island.index_coastline==0].iloc[dist_min,:].name\n",
    "        gridsdataframe.loc[index_next,'index_coastline']=index_teller\n",
    "        new_lat = gridsdataframe.loc[index_next,'index_lat']\n",
    "        new_lon = gridsdataframe.loc[index_next,'index_lon']\n",
    "        \n",
    "# Re-order the transition matrix and the gridsdataframe (re-index) so all islands are in the right order\n",
    "# add channel and fernandina island\n",
    "\n",
    "grids = gridsdataframe.copy(deep=True)\n",
    "\n",
    "index_ordering = np.argsort(np.array(gridsdataframe.index_coastline)-1)\n",
    "Tmatrix = transitionmatrix.copy()\n",
    "Tmatrix = Tmatrix[:,index_ordering]\n",
    "Tmatrix = Tmatrix[index_ordering,:]\n",
    "\n",
    "# order all other matrixes\n",
    "TM1 = TM1[:,index_ordering]\n",
    "TM1 = TM1[index_ordering,:]\n",
    "TM2 = TM2[:,index_ordering]\n",
    "TM2 = TM2[index_ordering,:]\n",
    "TM10 = TM10[:,index_ordering]\n",
    "TM10 = TM10[index_ordering,:]\n",
    "TM26 = TM26[:,index_ordering]\n",
    "TM26 = TM26[index_ordering,:]\n",
    "TM35 = TM35[:,index_ordering]\n",
    "TM35 = TM35[index_ordering,:]\n",
    "TM100 = TM100[:,index_ordering]\n",
    "TM100 = TM100[index_ordering,:]\n",
    "TM_all = TM_all[:,index_ordering]\n",
    "TM_all = TM_all[index_ordering,:]\n",
    "TM_half = TM_half[:,index_ordering]\n",
    "TM_half = TM_half[index_ordering,:]\n",
    "\n",
    "grids = grids.sort_values(by=['index_coastline'],ignore_index=True)\n",
    "grids.iloc[[channel_gridID],7]=11\n",
    "grids.iloc[[fernandina_gridID],7]=10\n",
    "\n",
    "# move Fernandina and channel to the end\n",
    "\n",
    "new_order = np.arange(0,len(gridsdataframe))\n",
    "new_order[channel_gridID] = new_order[channel_gridID]+2000\n",
    "new_order[fernandina_gridID] = new_order[fernandina_gridID]+1000\n",
    "\n",
    "grids.index_coastline=new_order\n",
    "index_ordering = np.argsort(np.array(new_order))\n",
    "Tmatrix = Tmatrix[:,index_ordering]\n",
    "Tmatrix = Tmatrix[index_ordering,:]\n",
    "\n",
    "# order all other matrixes\n",
    "TM1 = TM1[:,index_ordering]\n",
    "TM1 = TM1[index_ordering,:]\n",
    "TM2 = TM2[:,index_ordering]\n",
    "TM2 = TM2[index_ordering,:]\n",
    "TM10 = TM10[:,index_ordering]\n",
    "TM10 = TM10[index_ordering,:]\n",
    "TM26 = TM26[:,index_ordering]\n",
    "TM26 = TM26[index_ordering,:]\n",
    "TM35 = TM35[:,index_ordering]\n",
    "TM35 = TM35[index_ordering,:]\n",
    "TM100 = TM100[:,index_ordering]\n",
    "TM100 = TM100[index_ordering,:]\n",
    "TM_all = TM_all[:,index_ordering]\n",
    "TM_all = TM_all[index_ordering,:]\n",
    "TM_half = TM_half[:,index_ordering]\n",
    "TM_half = TM_half[index_ordering,:]\n",
    "\n",
    "grids = grids.sort_values(by=['index_coastline'],ignore_index=True)\n",
    "new_index_coastline = np.arange(1,len(grids)+1)\n",
    "grids.index_coastline = new_index_coastline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 1: map\n",
    "\n",
    "Map of the main islands in the Galapagos Marine Reserve. All port locations are indicated by diamond markers and virtual particle pathways (blue lines) show all existing connections via ocean currents using the MITgcm model simulation from Puerto Ayora (blue diamond) to other coastlines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#------ Make map ------#\n",
    "\n",
    "def make_map(projection=ccrs.PlateCarree()):\n",
    "    fig, ax = plt.subplots(figsize=(9, 8),\n",
    "                           subplot_kw=dict(projection=projection))\n",
    "    gl = ax.gridlines(draw_labels=False)\n",
    "    gl.xlabels_bottom = gl.ylabels_left = True\n",
    "    gl.xformatter = LONGITUDE_FORMATTER\n",
    "    gl.yformatter = LATITUDE_FORMATTER\n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = make_map(projection=ccrs.PlateCarree())\n",
    "\n",
    "extent = [-92, -89, -1.75, 1]\n",
    "ax.set_extent(extent)\n",
    "\n",
    "shp = shapereader.Reader('input/GSHHS_f_L1_Galapagos.shp')\n",
    "for record, geometry in zip(shp.records(), shp.geometries()):\n",
    "    ax.add_geometries([geometry], ccrs.PlateCarree(), facecolor=(4/255,138/255,112/255),\n",
    "                      edgecolor='black')\n",
    "\n",
    "# get colors for each island\n",
    "\n",
    "norm = colors.Normalize(vmin=0, vmax=max(grids.island_number))\n",
    "cmap = cm.get_cmap('copper')\n",
    "\n",
    "legend_colors=[]\n",
    "for i in range(max(grids.island_number)+1):\n",
    "    legend_colors.append(colors.to_hex(np.array(cmap(norm(int(i)),bytes=True) )/255))\n",
    "\n",
    "# Add tourist and port locations\n",
    "\n",
    "ax.scatter(lon_ports[3],lat_ports[3],s=100,\n",
    "           facecolor='#6fa8dc',\n",
    "           edgecolor='k',\n",
    "           marker='d',\n",
    "           zorder=202,\n",
    "           label='Puerto Ayora')\n",
    "ax.scatter(lon_ports,lat_ports,s=100,\n",
    "           facecolor='#e69138',\n",
    "           edgecolor='k',\n",
    "           marker='d',\n",
    "           zorder=201,\n",
    "           label='(Air)ports')\n",
    "\n",
    "ax.plot([0],[0],color= '#3d85c6',linewidth=0.7, label='Virtual particle pathways from Puerto Ayora')\n",
    "\n",
    "ax.legend()\n",
    "    \n",
    "# Add names of islands\n",
    "island_names = ['Isabela',\n",
    "                'Floreana',\n",
    "                'Española',\n",
    "                'Santa Cruz',\n",
    "                'Santa Fé',\n",
    "                'San Cristóbal',\n",
    "                'Santiago',\n",
    "                'Pinta',\n",
    "                'Marchena',\n",
    "                'Fernandina',\n",
    "                'Bolivar Channel']\n",
    "\n",
    "ax.text(-91.3, -0.85, island_names[0], color='w',fontsize='13', weight='bold')\n",
    "ax.text(-90.93, -1.4, island_names[1], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-89.6, -1.47, island_names[2], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-90.18, -0.53, island_names[3], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-90.3, -0.93, island_names[4], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-89.7, -1.1, island_names[5], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-90.7, -0.17, island_names[6], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-91.07, 0.55, island_names[7], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-91.02, 0.30, island_names[8], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-91.99, -0.25, island_names[9], color='k',fontsize='13', weight='bold')\n",
    "ax.text(-91.99, -0.62, island_names[10], color='k',fontsize='13', weight='bold')\n",
    "ax.plot([-91.3, -91.35],[-0.55, -0.36],c='k',linewidth=0.5)\n",
    "\n",
    "# Add trajectories from Puerto Ayora\n",
    "for i in range(lons.shape[0]):\n",
    "    ax.plot(lons[i,:],lats[i,:],color= '#3d85c6',linewidth=0.4, zorder=0, alpha=0.7)\n",
    "     \n",
    "plt.savefig('figures/islands_map.png',dpi=300,facecolor='#ffffff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 2: Connectivity matrix and beaching sensitivity\n",
    "\n",
    "The transition matrix (a) giving the probability that a particle starting at a source coastal node (y-axis) arrives at a sink coastal node (x-axis) for $\\lambda_B$= 5 days, and the probability (b) that a particle starting at a source coastal node is not beaching within 60 days and therefore \"lost\" to the ocean, including the sensitivity of this loss to changes in the beaching timescale (colored lines) compared to the reference simulation (black line). The different islands are delimited by horizontal and vertical grey lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# make figure\n",
    "\n",
    "logmatrix=Tmatrix.copy()\n",
    "logmatrix[logmatrix==0]=np.nan\n",
    "\n",
    "island_names = ['Isabela',\n",
    "                'Floreana',\n",
    "                'Española',\n",
    "                'Santa Cruz',\n",
    "                'Santa Fé',\n",
    "                'San Cristóbal',\n",
    "                'Santiago',\n",
    "                'Pinta',\n",
    "                'Marchena',\n",
    "                'Fernandina',\n",
    "                'Channel']\n",
    "\n",
    "fig = plt.figure(figsize=(14,9))\n",
    "gs = fig.add_gridspec(2, 2, width_ratios=[1, 0.5], height_ratios = [0.05,1])\n",
    "\n",
    "# plot transitionmatrix\n",
    "ax1 = fig.add_subplot(gs[1,0])\n",
    "im = ax1.pcolor(logmatrix/1800*100, \n",
    "                norm=colors.LogNorm(vmin=0.1, vmax=10),\n",
    "                cmap='magma',\n",
    "                shading='auto')\n",
    "\n",
    "# add colorbar\n",
    "ax2 = fig.add_subplot(gs[0,0])\n",
    "cbr = plt.colorbar(im, cax=ax2, extend='max', orientation='horizontal')\n",
    "cbr.set_label('Probability (%)', fontsize=14)\n",
    "cbr.ax.tick_params(labelsize=12)\n",
    "#ax2.set_title('Probability (%)', fontsize=14)\n",
    "\n",
    "# plot % of particles lost to the ocean\n",
    "col = plt.cm.copper(np.linspace(0.1,1,6,endpoint=True))\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1,1])\n",
    "ngrids=np.arange(0,len(grids))\n",
    "land_frac5 = 100-np.sum(Tmatrix,axis=1)/p_total*100\n",
    "land_frac1 = 100-np.sum(TM1,axis=1)/p_total*100\n",
    "land_frac2 = 100-np.sum(TM2,axis=1)/p_total*100\n",
    "land_frac10 = 100-np.sum(TM10,axis=1)/p_total*100\n",
    "land_frac26 = 100-np.sum(TM26,axis=1)/p_total*100\n",
    "land_frac35 = 100-np.sum(TM35,axis=1)/p_total*100\n",
    "land_frac100 = 100-np.sum(TM100,axis=1)/p_total*100\n",
    "ax3.plot(land_frac1,ngrids, c=col[0], linewidth=0.4, label = '$\\lambda$ = 1 d')\n",
    "ax3.plot(land_frac2,ngrids, c=col[1], linewidth=0.4, label = '$\\lambda$ = 2 d')\n",
    "ax3.plot(land_frac5,ngrids, c='k', linewidth=1.3, label = '$\\lambda$ = 5 d')\n",
    "ax3.plot(land_frac10,ngrids, c=col[2], linewidth=0.4, label = '$\\lambda$ = 10 d')\n",
    "ax3.plot(land_frac26,ngrids, c=col[3],linewidth=0.4, label = '$\\lambda$ = 26 d')\n",
    "ax3.plot(land_frac35,ngrids, c=col[4], linewidth=0.4, label = '$\\lambda$ = 35 d')\n",
    "ax3.set_xlabel('Connection to ocean sink (%)', fontsize=14)\n",
    "ax3.set_ylim([0,len(ngrids)])\n",
    "ax3.set_xlim([0,100])\n",
    "ax3.set_yticks([])\n",
    "ax3.tick_params(axis=\"x\", labelsize=12)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[0,1])\n",
    "ax4.axis('off')\n",
    "leg = ax3.legend(bbox_to_anchor=(1, 1), prop={\"size\":12}, ncol=2, bbox_transform=ax4.transAxes)\n",
    "\n",
    "for i,legobj in enumerate(leg.legendHandles):\n",
    "    if i == 2:\n",
    "        legobj.set_linewidth(2.0)\n",
    "    else:\n",
    "        legobj.set_linewidth(1.0)\n",
    "\n",
    "# add island separation lines\n",
    "nempty=np.zeros(len(grids))\n",
    "check = []\n",
    "yticks = []\n",
    "inames = []\n",
    "for i in range(len(grids)):\n",
    "    island_number = grids.island_number[i]\n",
    "    if ~np.any(check==island_number):\n",
    "        ax1.plot(ngrids,nempty+i,c='#5b5b5b',linewidth=0.8)\n",
    "        ax1.plot(nempty+i,ngrids,c='#5b5b5b',linewidth=0.8)       \n",
    "        ax3.plot([0,100],[i,i], c='#000000', linewidth=0.6)\n",
    "        check.append(island_number)\n",
    "        yticks.append(i)\n",
    "        inames.append(island_names[island_number-1])\n",
    "        \n",
    "# remove numbers\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0)\n",
    "ax1.set_yticks([])\n",
    "ax1.set_xticks([])\n",
    "\n",
    "# Add names to axis instead of numbers\n",
    "yticks.append(ngrids[-1])\n",
    "for i in range(len(yticks)-1):\n",
    "    loctext = (yticks[i]+yticks[i+1]-1)/2\n",
    "    ax1.text(-3, loctext, inames[i], fontsize=12, horizontalalignment='right', verticalalignment='center', rotation = 0)\n",
    "    ax1.text(loctext, -3, inames[i], fontsize=12, horizontalalignment='center', verticalalignment='top', rotation = -90)\n",
    "    \n",
    "ax1.text(-50, ngrids[-1]/2, 'source', rotation = 90, fontsize=14)\n",
    "ax1.text(ngrids[-1]/2, -50,'sink', fontsize=14) \n",
    "\n",
    "ax1.text(3, ngrids[-1]+7, 'a)', fontsize=16)\n",
    "ax3.text(1, ngrids[-1]+7, 'b)', fontsize=16)\n",
    "\n",
    "# Save figure\n",
    "#plt.tight_layout()\n",
    "#plt.savefig('figures/connectivity_matrix.png',dpi=300,facecolor='#ffffff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 3: Source Sink Distribution\n",
    "\n",
    "Source (a) and sink (c) distribution of macroplastic connectivity between the Galapagos Islands, specifying the relative connectivity to/from another island (green), to/from the same island (yellow), to/from the same location (brown) and to the ocean (blue). The total percentage of particles arriving at each location (node) is shown in panel b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Figure of source and sink distribution\n",
    "\n",
    "x = np.arange(0,c_total)\n",
    "fig, ax = plt.subplots(3,1, figsize=(12,12), sharex=True)\n",
    "island_number = np.array(grids.island_number)\n",
    "\n",
    "# Sink distribution - stacked bar graph\n",
    "\n",
    "ocean_frac = 100 - np.sum(Tmatrix,axis=1)/p_total*100\n",
    "long_frac = []\n",
    "self_frac = []\n",
    "\n",
    "for i,island in enumerate(island_number):   \n",
    "    indx=np.where(island_number != island)\n",
    "    long_frac.append(np.sum(Tmatrix[i,indx])/p_total*100)\n",
    "    self_frac.append(Tmatrix[i,i]/p_total*100)\n",
    "        \n",
    "short_frac = 100 - ocean_frac - long_frac - self_frac\n",
    "\n",
    "ax[0].bar(x, long_frac, color='#4e8500', width=1, label = 'to another island')\n",
    "ax[0].bar(x, self_frac, bottom=long_frac, color='#7b4a18', width=1, label = 'to same location')\n",
    "ax[0].bar(x, short_frac, bottom=[i+j for i,j in zip(long_frac, self_frac)], color='#f7da8b', width=1, label='to same island')\n",
    "ax[0].bar(x, ocean_frac, bottom=[i+j+z for i,j,z in zip(short_frac, long_frac, self_frac)], color='#d1f3ff', width=1, label='to ocean')\n",
    "\n",
    "ax[0].set_xlim([0,c_total-1])\n",
    "ax[0].set_ylim([0,100])\n",
    "ax[0].legend(loc='upper left', prop={\"size\":12}, ncol=2)\n",
    "for i in yticks[1:-1]:\n",
    "    ax[0].plot([i,i],[0,100],c='k',linewidth=1)\n",
    "\n",
    "ax[0].set_title('a) Sink distribution', fontsize=12)\n",
    "ax[0].set_ylabel('Sink distribution at each node (%)', fontsize=12)\n",
    "ax[0].set_xticks([])\n",
    "ax[0].tick_params(axis='x',direction='in') \n",
    "    \n",
    "# Source distribution - stacked bar graph\n",
    "    \n",
    "long_frac = []\n",
    "short_frac = []\n",
    "self_frac = []\n",
    "total_arrival = []\n",
    "for i, island in enumerate(island_number):\n",
    "    indx_long=np.where(island_number != island)\n",
    "    indx_short=np.where(island_number == island)\n",
    "    long_frac.append(np.sum(Tmatrix[indx_long,i]))\n",
    "    short_frac.append(np.sum(Tmatrix[indx_short,i]))\n",
    "    self_frac.append(np.sum(Tmatrix[i,i]))\n",
    "    total_arrival.append(np.sum(Tmatrix[:,i]))\n",
    "\n",
    "i=6\n",
    "print('percentage arriving at island ' + island_names[i-1] + ' from elsewhere is ' + str(np.sum(np.array(long_frac)[island_number==i])/np.sum(np.array(total_arrival)[island_number==i])))    \n",
    "    \n",
    "long_frac = np.array(long_frac)/np.array(total_arrival)*100\n",
    "short_frac = np.array(short_frac)/np.array(total_arrival)*100\n",
    "self_frac = np.array(self_frac)/np.array(total_arrival)*100\n",
    "\n",
    "\n",
    "ax[2].bar(x, long_frac, color='#4e8500', width=1, label = 'from another island')\n",
    "ax[2].bar(x, self_frac, bottom=long_frac, color='#7b4a18', width=1, label = 'from same location')\n",
    "ax[2].bar(x, short_frac, bottom=[i+j for i,j in zip(long_frac, self_frac)], color='#f7da8b', width=1, label='from same island')\n",
    "\n",
    "\n",
    "ax[2].set_xlim([0,c_total-1])\n",
    "ax[2].set_ylim([0,100])\n",
    "ax[2].legend(loc='upper left', prop={\"size\":12}, ncol=2)\n",
    "for i in yticks[1:-1]:\n",
    "    ax[2].plot([i,i],[0,100],c='k',linewidth=1)\n",
    "\n",
    "ax[2].set_title('c) Source distribution', fontsize=12)\n",
    "ax[2].set_ylabel('Source distribution at each node (%)', fontsize=12)\n",
    "\n",
    "# Set ticks\n",
    "\n",
    "island_names = ['Isabela',\n",
    "                'Floreana',\n",
    "                'Española',\n",
    "                'Santa Cruz',\n",
    "                'Santa Fé',\n",
    "                'San Cristóbal',\n",
    "                'Santiago',\n",
    "                'Pinta',\n",
    "                'Marchena',\n",
    "                'Fernandina',\n",
    "                'Channel']\n",
    "\n",
    "ax[2].set_xticks([])\n",
    "ax[2].tick_params(axis='x',direction='in')\n",
    "\n",
    "for i in range(len(yticks)-1):\n",
    "    loctext = (yticks[i]+yticks[i+1]-1)/2\n",
    "    ax[2].text(loctext, -0.8, inames[i], fontsize=12, horizontalalignment='left', verticalalignment='top', rotation = -30)\n",
    "   \n",
    " \n",
    "fig.subplots_adjust(hspace=0)    \n",
    "    \n",
    "# Figure showing total arrival of particles at each node\n",
    "\n",
    "ax[1].yaxis.tick_right()\n",
    "ax[1].set_ylabel('Percentage arriving at each node (%)', fontsize=12)\n",
    "ax[1].yaxis.set_label_position(\"right\")\n",
    "\n",
    "ax[1].plot(x,np.sum(Tmatrix,axis=0)/(c_total*p_total)*100,c='k')\n",
    "ax[1].set_ylim([-0.15, 1.15])\n",
    "ax[1].plot(x,np.zeros_like(x),c='k',linewidth=0.4)\n",
    "ax[1].plot(x,np.zeros_like(x)+1,c='k',linewidth=0.4)\n",
    "\n",
    "ax[1].set_title('b) Total arriving', y=1.0, pad=-17, fontsize=12)\n",
    "\n",
    "for i in yticks[1:-1]:\n",
    "    ax[1].plot([i,i],[0,1],c='k',linewidth=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/sourcesink_distribution.png',dpi=300,facecolor='#ffffff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to calculate centralities and impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     135,
     192,
     235,
     286
    ]
   },
   "outputs": [],
   "source": [
    "# Function to calculate all centralities\n",
    "\n",
    "class CIs:\n",
    "    \n",
    "    def __init__(self, Tmatrix, grids, p_total, lon_ports, lat_ports):\n",
    "        self.tm = Tmatrix\n",
    "        self.ptotal = p_total\n",
    "        self.clon = grids.lon\n",
    "        self.clat = grids.lat \n",
    "        self.lon_civil = lon_ports\n",
    "        self.lat_civil = lat_ports\n",
    "          \n",
    "        # initialize parameters of interest\n",
    "        self.centralities = {}    \n",
    "        self.graph = None\n",
    "        \n",
    "    def make_graph(self):\n",
    "        \n",
    "        # initialize graph\n",
    "        pgraph = nx.DiGraph()\n",
    "        pgraphR = nx.DiGraph()\n",
    "        nodenames = []\n",
    "        locations = []\n",
    "        \n",
    "        # Add nodes\n",
    "        for i in range(self.tm.shape[0]):\n",
    "            lon =  self.clon[i]\n",
    "            lat =  self.clat[i]\n",
    "            location = (lon,lat)\n",
    "\n",
    "            pgraph.add_node(i, pos = location)\n",
    "            pgraphR.add_node(i, pos = locations)\n",
    "\n",
    "        # Add edges\n",
    "        for release_node in pgraph.nodes():\n",
    "            for beach_node in pgraph.nodes():\n",
    "                weight = self.tm[release_node, beach_node]        \n",
    "                if weight > 0:\n",
    "                    weightlog = -np.log(self.tm[release_node, beach_node]/self.ptotal)\n",
    "                    pgraph.add_edge(release_node,\n",
    "                                    beach_node,\n",
    "                                    weightpath = weight,\n",
    "                                    weightlogpath = weightlog)\n",
    "                    pgraphR.add_edge(beach_node,\n",
    "                                    release_node,\n",
    "                                    weightpath = weight,\n",
    "                                    weightlogpath = weightlog)\n",
    "        \n",
    "        self.graph = pgraph\n",
    "        self.graphR = pgraphR   \n",
    "    \n",
    "    def calculate_betweenness(self):\n",
    "        \n",
    "        betweenness = nx.centrality.betweenness_centrality(self.graph, weight = 'weightlogpath')\n",
    "        self.centralities['betweenness'] = betweenness.values()\n",
    "        \n",
    "    def calculate_pagerank(self):\n",
    "\n",
    "        pagerank = nx.pagerank(self.graph, alpha = 0.85, max_iter = 200, tol = 1e-6, weight = 'weightlogpath')  \n",
    "        self.centralities['PRin'] = pagerank.values()\n",
    "        pagerank = nx.pagerank(self.graphR, alpha = 0.85, max_iter = 200, tol = 1e-6, weight = 'weightlogpath')  \n",
    "        self.centralities['PRout'] = pagerank.values()\n",
    "    \n",
    "    def calculate_retention(self):\n",
    "        \n",
    "        retention = np.zeros(self.tm.shape[0])\n",
    "\n",
    "        for i in range(self.tm.shape[0]):\n",
    "            retention[i] = self.tm[i,i]/self.ptotal\n",
    "        \n",
    "        self.centralities['retention'] = retention\n",
    "            \n",
    "    def calculate_loss(self):\n",
    "        \n",
    "        loss = 1 - np.sum(self.tm,axis=1)/self.ptotal\n",
    "        self.centralities['loss'] = loss\n",
    "        \n",
    "    def calculate_beaching(self):\n",
    "        \n",
    "        self.centralities['beaching'] = np.sum(self.tm, axis=1)/self.ptotal - self.centralities['retention']\n",
    "        \n",
    "    def calculate_SSI(self):\n",
    "        \n",
    "        SSIsink = np.zeros(self.tm.shape[0]) #net sink\n",
    "        SSIsource = np.zeros(self.tm.shape[0]) #net source\n",
    "        \n",
    "        for i in range(self.tm.shape[0]):\n",
    "            SSIsink[i] = (np.sum(self.tm[:,i]) - np.sum(self.tm[i,:]))/(np.sum(self.tm[:,i]) + np.sum(self.tm[i,:]))\n",
    "            SSIsource[i] = -1*SSIsink[i]\n",
    "        self.centralities['SSIsink'] = SSIsink\n",
    "        self.centralities['SSIsource'] = SSIsource\n",
    "        \n",
    "    def calculate_SiD(self): #sink diversity (diversity of outgoing edges)\n",
    "        \n",
    "        SiD = np.zeros(self.tm.shape[0])\n",
    "        total_outward = np.sum(self.tm, axis=1)     \n",
    "               \n",
    "        for i in range(self.tm.shape[0]): \n",
    "            for j in range(self.tm.shape[0]):\n",
    "                if self.tm[i,j]>0:\n",
    "                    SiD[i] += -1*self.tm[i,j]/total_outward[i]*np.log(self.tm[i,j]/total_outward[i])\n",
    "        \n",
    "        self.centralities['SiD'] = SiD\n",
    "        \n",
    "    def calculate_SoD(self): #source diversity (diversity of incoming edges)\n",
    "\n",
    "        SoD = np.zeros(self.tm.shape[0])\n",
    "        total_inward = np.sum(self.tm, axis=0)\n",
    "        \n",
    "        for j in range(self.tm.shape[0]):\n",
    "            for i in range(self.tm.shape[0]):\n",
    "                if self.tm[i,j]>0:\n",
    "                    SoD[j] += -1*self.tm[i,j]/total_inward[j]*np.log(self.tm[i,j]/total_inward[j])\n",
    "\n",
    "        self.centralities['SoD'] = SoD\n",
    "        \n",
    "    def get_centralities(self):\n",
    "        \n",
    "        self.calculate_retention()\n",
    "        self.calculate_loss()\n",
    "        self.calculate_beaching() \n",
    "        self.calculate_SSI() \n",
    "        self.calculate_SiD()\n",
    "        self.calculate_SoD()\n",
    "        \n",
    "        print('done with centralities')\n",
    "        \n",
    "        self.make_graph()\n",
    "        print('made graph, start betweenness')\n",
    "        self.calculate_betweenness()\n",
    "        print('calculate pagerank')\n",
    "        self.calculate_pagerank()\n",
    "        \n",
    "# Function to calculate impact metrics homogeneous initialisation\n",
    "\n",
    "def calculate_impact(centrality_ranked, Tmatrix, centr_name, iters, threshold, p_total):\n",
    "    \n",
    "    Tprob = Tmatrix.copy()\n",
    "    ocean_source = np.zeros(Tprob.shape[0])\n",
    "    Tprob=np.vstack((Tprob,ocean_source))\n",
    "    ocean_sink = p_total - np.sum(Tprob,axis=1)\n",
    "    Tprob=np.column_stack((Tprob,ocean_sink))\n",
    "    Tprob = Tprob/p_total\n",
    "    \n",
    "    # get order of removal\n",
    "    removal_order = np.array(centrality_ranked,dtype='int')-1\n",
    "    removal_order = np.argsort(removal_order)\n",
    "    \n",
    "    iterations = np.arange(0,iters)\n",
    "    benchmark = np.linspace(0,1,Tprob.shape[0])\n",
    "    \n",
    "    # diagnostics\n",
    "    benefit = [0]\n",
    "    fraction_ocean = [np.nan]\n",
    "    LBL = [1]\n",
    "    iterations_metric = [np.nan]\n",
    "    remove_idx = []\n",
    "\n",
    "    for b,r in enumerate(removal_order):\n",
    "\n",
    "        remove_idx.append(r)\n",
    "\n",
    "        plastic = np.zeros(Tprob.shape[0])+1 # all plastic on land\n",
    "        plastic[-1]=0 # no plastic in the ocean at start\n",
    "        total = np.sum(plastic) # total plastic at start\n",
    "\n",
    "        ocean = []\n",
    "        removed = []\n",
    "        land = []\n",
    "\n",
    "        for i in iterations:\n",
    "            ocean.append(plastic[-1]/total)\n",
    "            land.append(np.sum(plastic[:-1])/total)\n",
    "            removed.append((total-np.sum(plastic))/total)\n",
    "            plastic[remove_idx] = 0 #remove plastic\n",
    "            plastic = np.dot(plastic,Tprob)\n",
    "\n",
    "        benefit.append(removed[-1]-benchmark[b])\n",
    "        LBL.append(land[-1])\n",
    "        \n",
    "        threshold_onland = (1-np.nanmin(land))*threshold # if using left on land to calculate steady state\n",
    "        f = interp1d((1-np.array(land)), iterations)\n",
    "              \n",
    "        #threshold_oceanloss = np.nanmax(ocean)*threshold # if using ocean loss to calculate steady state\n",
    "        #f = interp1d(ocean, iterations)    \n",
    "        iterations_metric.append(f(threshold_onland)) #number of iterations needed to reach 'threshold'% of final percentage on land\n",
    "        \n",
    "    return benefit, LBL, iterations_metric\n",
    "\n",
    "\n",
    "# Function to calculate impact metrics homogeneous initialisation - combining best scoring centralities\n",
    "\n",
    "def calculate_impact_combine(remove_idx, Tmatrix, iters, threshold, p_total):\n",
    "    \n",
    "    Tprob = Tmatrix.copy()\n",
    "    ocean_source = np.zeros(Tprob.shape[0])\n",
    "    Tprob=np.vstack((Tprob,ocean_source))\n",
    "    ocean_sink = p_total - np.sum(Tprob,axis=1)\n",
    "    Tprob=np.column_stack((Tprob,ocean_sink))\n",
    "    Tprob = Tprob/p_total\n",
    "    \n",
    "    iterations = np.arange(0,iters)\n",
    "    benchmark = np.linspace(0,1,Tprob.shape[0])\n",
    "\n",
    "\n",
    "    plastic = np.zeros(Tprob.shape[0])+1 # all plastic on land\n",
    "    plastic[-1]=0 # no plastic in the ocean at start\n",
    "    total = np.sum(plastic) # total plastic at start\n",
    "\n",
    "    ocean = []\n",
    "    removed = []\n",
    "    land = []\n",
    "    \n",
    "    for i in iterations:\n",
    "        ocean.append(plastic[-1]/total)\n",
    "        land.append(np.sum(plastic[:-1])/total)\n",
    "        removed.append((total-np.sum(plastic))/total)\n",
    "        plastic[remove_idx] = 0 #remove plastic\n",
    "        plastic = np.dot(plastic,Tprob)\n",
    "\n",
    "    b = len(remove_idx)\n",
    "    benefit = removed[-1]-benchmark[b]\n",
    "    LBL = land[-1]\n",
    "        \n",
    "    threshold_onland = (1-np.nanmin(land))*threshold # if using left on land to calculate steady state\n",
    "    f = interp1d((1-np.array(land)), iterations)\n",
    "              \n",
    "    #threshold_oceanloss = np.nanmax(ocean)*threshold # if using ocean loss to calculate steady state\n",
    "    #f = interp1d(ocean, iterations)    \n",
    "    iterations_metric = f(threshold_onland) #number of iterations needed to reach 'threshold'% of final percentage on land\n",
    "        \n",
    "    return benefit, LBL, iterations_metric\n",
    "\n",
    "# Function to calculate impact metrics different initial distribution\n",
    "\n",
    "def calculate_impact_distr(centrality_ranked, Tmatrix, centr_name, iters, threshold, p_total, initial_distribution, coastline_fraction):\n",
    "    \n",
    "    Tprob = Tmatrix.copy()\n",
    "    ocean_source = np.zeros(Tprob.shape[0])\n",
    "    Tprob=np.vstack((Tprob,ocean_source))\n",
    "    ocean_sink = p_total - np.sum(Tprob,axis=1)\n",
    "    Tprob=np.column_stack((Tprob,ocean_sink))\n",
    "    Tprob = Tprob/p_total\n",
    "    \n",
    "    # get order of removal\n",
    "    removal_order = np.array(centrality_ranked,dtype='int')-1\n",
    "    removal_order = np.argsort(removal_order)\n",
    "    remove_idx = removal_order[:coastline_fraction]\n",
    "    removal_order2 = np.argsort(initial_distribution)\n",
    "    remove_idx2 = removal_order2[-coastline_fraction:]\n",
    "    \n",
    "    # useful\n",
    "    iterations = np.arange(0,iters)\n",
    "    coastline = np.linspace(0,1,Tprob.shape[0]) # fraction of coastline cleaned\n",
    "\n",
    "    # initial distribution - cleaning based on centrality\n",
    "    plastic = np.copy(initial_distribution)\n",
    "    plastic[-1] = 0 # make sure there is no plastic in the ocean at start\n",
    "    total = np.sum(plastic) # total plastic at start\n",
    "    removed = []\n",
    "\n",
    "    for i in iterations:\n",
    "        removed.append((total-np.sum(plastic))/total)\n",
    "        plastic[remove_idx] = 0 #remove plastic\n",
    "        plastic = np.dot(plastic,Tprob)\n",
    "   \n",
    "    total_removed = removed[-1]\n",
    "        \n",
    "    # initial distribution - cleaning if knowing where high MPD\n",
    "    plastic = np.copy(initial_distribution)\n",
    "    plastic[-1] = 0 # make sure there is no plastic in the ocean at start\n",
    "    total = np.sum(plastic) # total plastic at start\n",
    "    removed = []\n",
    "\n",
    "    for i in iterations:\n",
    "        removed.append((total-np.sum(plastic))/total)\n",
    "        plastic[remove_idx2] = 0 #remove plastic\n",
    "        plastic = np.dot(plastic,Tprob)\n",
    "   \n",
    "    optimal_removed = removed[-1]\n",
    "    \n",
    "    \n",
    "    return total_removed, optimal_removed\n",
    "\n",
    "# Function to get all centrality rankings\n",
    "\n",
    "def get_centrality_rankings(transition_matrix, grids, p_total, lon_ports, lat_ports):\n",
    "\n",
    "    centr_obj = CIs(transition_matrix, grids, p_total, lon_ports, lat_ports)\n",
    "    centr_obj.get_centralities()\n",
    "    centr = pd.DataFrame(centr_obj.centralities)\n",
    "    ranking = centr.rank(ascending=False) #highest centrality should be cleaned first\n",
    "    \n",
    "    return centr_obj, centr, ranking\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Explanation for finding steady state (comment 1)\n",
    "\n",
    "n = 1\n",
    "centr_names = ranking.columns.values.tolist()\n",
    "centrality_ranked = ranking[centr_names[n]]\n",
    "print(centr_names[n])\n",
    "iters = 30\n",
    "threshold = 0.99\n",
    "threshold_l = 0.99\n",
    "\n",
    "Tprob = Tmatrix.copy()\n",
    "ocean_source = np.zeros(Tprob.shape[0])\n",
    "Tprob=np.vstack((Tprob,ocean_source))\n",
    "ocean_sink = p_total - np.sum(Tprob,axis=1)\n",
    "Tprob=np.column_stack((Tprob,ocean_sink))\n",
    "Tprob = Tprob/p_total\n",
    "\n",
    "# get order of removal\n",
    "removal_order = np.array(centrality_ranked,dtype='int')-1\n",
    "removal_order = np.argsort(removal_order)\n",
    "\n",
    "iterations = np.arange(0,iters)\n",
    "benchmark = np.linspace(0,1,Tprob.shape[0])\n",
    "\n",
    "# make figures\n",
    "fig,ax = plt.subplots(2,1, figsize=(8,10))\n",
    "\n",
    "b = int(len(centrality_ranked)*0.1)\n",
    "r = removal_order[b]\n",
    "remove_idx = removal_order[:b]\n",
    "    \n",
    "plastic = np.zeros(Tprob.shape[0])+1 # all plastic on land\n",
    "plastic[-1]=0 # no plastic in the ocean at start\n",
    "total = np.sum(plastic) # total plastic at start\n",
    "\n",
    "ocean = []\n",
    "removed = []\n",
    "land = []\n",
    "\n",
    "for i in iterations:\n",
    "    ocean.append(plastic[-1]/total)\n",
    "    land.append(np.sum(plastic[:-1])/total)\n",
    "    removed.append((total-np.sum(plastic))/total)\n",
    "    plastic[remove_idx] = 0 #remove plastic\n",
    "    plastic = np.dot(plastic,Tprob)\n",
    "\n",
    "\n",
    "ax[0].plot(iterations,ocean,c='b',label='% in ocean')\n",
    "ax[0].plot(iterations,land,c='g',label='% on land')\n",
    "ax[0].plot(iterations,removed,c='brown', label='% removed')\n",
    "ax[0].set_ylim([0,1])\n",
    "ax[0].set_ylabel('fraction')\n",
    "ax[0].set_xlabel('iteration')\n",
    "ax[0].set_title('Distribution of particles across reservoirs - 10% repeated cleanup effort')    \n",
    "    \n",
    "threshold_oceanloss = np.nanmax(ocean)*threshold\n",
    "f = interp1d(ocean, iterations)    \n",
    "ax[0].plot([f(threshold_oceanloss), f(threshold_oceanloss)], [0,1],'--b',label='steady state using ocean curve')\n",
    "\n",
    "threshold_oceanloss = (1-np.nanmin(land))*threshold_l\n",
    "f = interp1d((1-np.array(land)), iterations)    \n",
    "ax[0].plot([f(threshold_oceanloss), f(threshold_oceanloss)], [0,1],'-.g', label='steady state using land curve')\n",
    "\n",
    "ax[0].legend()\n",
    "\n",
    "# second panel\n",
    "\n",
    "b = int(len(centrality_ranked)*0.9)\n",
    "r = removal_order[b]\n",
    "remove_idx = removal_order[:b]\n",
    "    \n",
    "plastic = np.zeros(Tprob.shape[0])+1 # all plastic on land\n",
    "plastic[-1]=0 # no plastic in the ocean at start\n",
    "total = np.sum(plastic) # total plastic at start\n",
    "\n",
    "ocean = []\n",
    "removed = []\n",
    "land = []\n",
    "\n",
    "for i in iterations:\n",
    "    ocean.append(plastic[-1]/total)\n",
    "    land.append(np.sum(plastic[:-1])/total)\n",
    "    removed.append((total-np.sum(plastic))/total)\n",
    "    plastic[remove_idx] = 0 #remove plastic\n",
    "    plastic = np.dot(plastic,Tprob)\n",
    "\n",
    "\n",
    "ax[1].plot(iterations,ocean,c='b',label='% in ocean')\n",
    "ax[1].plot(iterations,land,c='g',label='% on land')\n",
    "ax[1].plot(iterations,removed,c='brown', label='% removed')\n",
    "ax[1].set_ylim([0,1])\n",
    "ax[1].set_ylabel('fraction')\n",
    "ax[1].set_xlabel('iteration')\n",
    "ax[1].set_title('Distribution of particles across reservoirs - 90% repeated cleanup effort') \n",
    "\n",
    "threshold_oceanloss = np.nanmax(ocean)*threshold\n",
    "f = interp1d(ocean, iterations)    \n",
    "ax[1].plot([f(threshold_oceanloss), f(threshold_oceanloss)], [0,1],'--b',label='steady state using ocean curve')\n",
    "\n",
    "threshold_oceanloss = (1-np.nanmin(land))*threshold_l\n",
    "f = interp1d((1-np.array(land)), iterations)    \n",
    "ax[1].plot([f(threshold_oceanloss), f(threshold_oceanloss)], [0,1],'-.g', label='steady state using land curve')\n",
    "\n",
    "ax[1].legend()\n",
    "\n",
    "plt.savefig('figures/steadystate_calculation_comment1.png',dpi=300,facecolor='#ffffff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Sensitivity to number of particles (comment 1)\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "logmatrix=Tmatrix.copy()\n",
    "logmatrix[logmatrix==0]=np.nan\n",
    "\n",
    "logmatrix_half=TM_half.copy()\n",
    "logmatrix_half[logmatrix_half==0]=np.nan\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize=(16,8))\n",
    "\n",
    "# plot transitionmatrix\n",
    "im1 = ax[0].pcolor(logmatrix/1800*100, \n",
    "             norm=colors.LogNorm(vmin=0.1, vmax=10),\n",
    "             cmap='magma',\n",
    "             shading='auto')\n",
    "\n",
    "im2 = ax[1].pcolor(logmatrix_half/900*100, \n",
    "             norm=colors.LogNorm(vmin=0.1, vmax=10),\n",
    "             cmap='magma',\n",
    "             shading='auto')\n",
    "\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "fig.colorbar(im2, cax=cax, orientation='vertical')\n",
    "\n",
    "ax[0].set_title('Transition Matrix with all particles')\n",
    "ax[1].set_title('Transition Matrix with 0.5 x all particles')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/nparticles_comment1.png',dpi=300,facecolor='#ffffff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 4: Comparison of different centralities - impact\n",
    "\n",
    "A comparison of the impact metrics described in section 2.5 as a function of the fraction of coastline nodes cleaned for all centrality rankings (colored lines). The benefit metric (a) measures the difference (in %) between the total number of particles removed and the number of particles removed if there would have been zero connectivity between the different nodes. The Left Behind on Land metric (b) indicates how many particles are still on land after steady state is reached. The Iterations metric (c) shows how many iterations where needed to reach steady state and provides an indication for how often one should clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# make figure\n",
    "\n",
    "centr_obj, centr, ranking = get_centrality_rankings(Tmatrix, grids, p_total, lon_ports, lat_ports)\n",
    "\n",
    "centr_names = ranking.columns.values.tolist()\n",
    "coastline = np.linspace(0,1,Tmatrix.shape[0]+1)*100\n",
    "\n",
    "iters = 30 # should be big enough to always reach steady state\n",
    "threshold = 0.99 # to find when in steady state\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "colors = np.linspace(0,1,len(centr_names))\n",
    "fig, ax = plt.subplots(3,1, figsize=(10,10), sharex='all')\n",
    "\n",
    "for i,name in enumerate(centr_names):\n",
    "    \n",
    "    benefit, LBL, iterations_metric = calculate_impact(ranking[name], Tmatrix, name, iters, threshold, p_total)\n",
    "    ax[0].plot(coastline, np.array(benefit)*100, color=cmap(colors[i]), label=name)\n",
    "    ax[1].plot(coastline[1:], np.array(LBL[1:])*100, color=cmap(colors[i]))\n",
    "    ax[2].plot(coastline[:-1], iterations_metric[:-1], color=cmap(colors[i]), label=name)\n",
    "\n",
    "fig.subplots_adjust(hspace=0)  \n",
    "ax[0].legend(bbox_to_anchor=(1.01, 1.0), loc='upper left', prop={\"size\":12}, ncol=1)\n",
    "ax[2].set_xlim([0,100])\n",
    "ax[0].set_ylabel('Benefit (%)')\n",
    "ax[1].set_ylabel('Left Behind on Land (%)')\n",
    "ax[2].set_ylabel('Iterations needed to reach steady state')\n",
    "ax[2].set_xlabel('Fraction of coastline cleaned (%)')\n",
    "\n",
    "ax[0].text(96, 19.5, 'a)', fontsize=16)\n",
    "ax[1].text(96, 3.2, 'b)', fontsize=16)\n",
    "ax[2].text(96, 15.7, 'c)', fontsize=16)\n",
    "ax[2].set_ylim([0,17])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('figures/impact_centralities.png',dpi=300,facecolor='#ffffff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# make figure to compare combining 4 best scoring centralities (reviewer 3)\n",
    "\n",
    "#centr_obj, centr, ranking = get_centrality_rankings(Tmatrix, grids, p_total, lon_ports, lat_ports)\n",
    "\n",
    "centr_names = ranking.columns.values.tolist()\n",
    "coastline = np.linspace(0,1,Tmatrix.shape[0]+1)*100\n",
    "\n",
    "iters = 30 # should be big enough to always reach steady state\n",
    "threshold = 0.99 # to find when in steady state\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "colors = np.linspace(0,1,len(centr_names))\n",
    "fig, ax = plt.subplots(3,1, figsize=(10,10), sharex='all')\n",
    "\n",
    "for i,name in enumerate(centr_names):    \n",
    "    benefit, LBL, iterations_metric = calculate_impact(ranking[name], Tmatrix, name, iters, threshold, p_total)\n",
    "    ax[0].plot(coastline, np.array(benefit)*100, color='grey', linewidth=0.7)\n",
    "    ax[1].plot(coastline[1:], np.array(LBL[1:])*100, color='grey', linewidth=0.7)\n",
    "    ax[2].plot(coastline[:-1], iterations_metric[:-1], color='grey', linewidth=0.7)\n",
    "\n",
    "\n",
    "# New order based on 4 centralities\n",
    "order_RR = np.array(ranking[centr_names[0]],dtype='int')-1\n",
    "order_RR = np.argsort(order_RR)\n",
    "order_SSIsink = np.array(ranking[centr_names[3]],dtype='int')-1\n",
    "order_SSIsink = np.argsort(order_SSIsink)\n",
    "order_BC = np.array(ranking[centr_names[7]],dtype='int')-1\n",
    "order_BC = np.argsort(order_BC)\n",
    "order_PRin = np.array(ranking[centr_names[8]],dtype='int')-1\n",
    "order_PRin = np.argsort(order_PRin)\n",
    "\n",
    "nx = np.arange(1,len(remove_idx)+1,dtype='int')\n",
    "\n",
    "    # make new index and remove duplicates\n",
    "remove_idx = []\n",
    "for i in range(len(order_RR)):\n",
    "    remove_idx.append(order_RR[i])\n",
    "    remove_idx.append(order_SSIsink[i])\n",
    "    remove_idx.append(order_BC[i])\n",
    "    remove_idx.append(order_PRin[i])\n",
    "remove_idx = list(OrderedDict.fromkeys(remove_idx))\n",
    "\n",
    "benefit=[]\n",
    "LBL=[]\n",
    "iterations_metrix=[]\n",
    "for i in nx:\n",
    "    cbenefit, cLBL, citerations_metric = calculate_impact_combine(remove_idx[:i], Tmatrix, iters, threshold, p_total)\n",
    "    benefit.append(cbenefit*100)\n",
    "    LBL.append(cLBL*100)\n",
    "    iterations_metrix.append(citerations_metric)\n",
    "    \n",
    "ax[0].plot(coastline[1:], benefit, color='r', label='Combining 4 most promising centralities')\n",
    "ax[1].plot(coastline[1:], LBL, color='r')\n",
    "ax[2].plot(coastline[1:], iterations_metrix, color='r')\n",
    "ax[0].plot(coastline, np.zeros(len(coastline))-10,color='grey',linewidth=0.7,label='Individual centralities')\n",
    "\n",
    "\n",
    "fig.subplots_adjust(hspace=0)  \n",
    "ax[0].legend(bbox_to_anchor=(1.01, 1.0), loc='upper left', prop={\"size\":12}, ncol=1)\n",
    "ax[2].set_xlim([0,100])\n",
    "ax[0].set_ylabel('Benefit (%)')\n",
    "ax[1].set_ylabel('Left Behind on Land (%)')\n",
    "ax[2].set_ylabel('Iterations needed to reach steady state')\n",
    "ax[2].set_xlabel('Fraction of coastline cleaned (%)')\n",
    "ax[0].set_ylim([-1,22])\n",
    "\n",
    "ax[0].text(96, 19.5, 'a)', fontsize=16)\n",
    "ax[1].text(96, 3.2, 'b)', fontsize=16)\n",
    "ax[2].text(96, 15.7, 'c)', fontsize=16)\n",
    "ax[2].set_ylim([0,17])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figures/impact_combined_centralities.png',dpi=300,facecolor='#ffffff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 5: Impact limited cleaning effort\n",
    "\n",
    "The benefit of all centrality node rankings as a function of (a) how much of the initial plastic distribution is left on land and (b)how often the matrix multiplication needs to be performed to reach steady state. The impact metrics are calculated for when 5% (diamond marker) and 10% (star marker) of the coastline would be cleaned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# FIGURE - Impact optimalisation full matrix\n",
    "\n",
    "centr_obj, centr, ranking = get_centrality_rankings(Tmatrix, grids, p_total, lon_ports, lat_ports)\n",
    "\n",
    "centr_names = ranking.columns.values.tolist()\n",
    "coastline = np.linspace(0,1,Tmatrix.shape[0]+1)\n",
    "\n",
    "iters = 30 # should be big enough to always reach steady state\n",
    "threshold = 0.95 # to find when in steady state\n",
    "\n",
    "c10=int(0.1*len(coastline))\n",
    "c5=int(0.05*len(coastline))\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "colors = np.linspace(0,1,len(centr_names))\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,6), sharey='all')\n",
    "\n",
    "for i,name in enumerate(centr_names):\n",
    "    \n",
    "    benefit, LBL, iterations_metric = calculate_impact(ranking[name], Tmatrix, name, iters, threshold, p_total)\n",
    "    ax[0].plot(np.array(LBL[1:c10+1])*100,np.array(benefit[1:c10+1])*100, color=cmap(colors[i]),linewidth=1,zorder=0)\n",
    "    ax[0].scatter(LBL[c10]*100,benefit[c10]*100, s=150, color=cmap(colors[i]), marker='*',zorder=20)\n",
    "    ax[0].scatter(LBL[c5]*100,benefit[c5]*100, s=60, color=cmap(colors[i]), marker='D',zorder=20)\n",
    "    ax[1].plot(iterations_metric[0:c10+1],np.array(benefit[0:c10+1])*100, color=cmap(colors[i]),linewidth=1,label=name,zorder=0)\n",
    "    ax[1].scatter(iterations_metric[c10],benefit[c10]*100, s=150, color=cmap(colors[i]), marker='*',zorder=20)\n",
    "    ax[1].scatter(iterations_metric[c5],benefit[c5]*100, s=60, color=cmap(colors[i]), marker='D',zorder=20)\n",
    "\n",
    "# get some additional legends\n",
    "ax[0].scatter(-10,0, s=60, c='k', marker='D',label='5% cleanup effort')\n",
    "ax[0].scatter(-10,0, s=150, c='k', marker='*',label='10% cleanup effort')\n",
    "    \n",
    "    \n",
    "ax[1].legend(fontsize=12)\n",
    "ax[0].set_xlabel('Left Behind on Land (%)', fontsize=12)\n",
    "ax[1].set_xlabel('Number of iterations needed to reach steady state', fontsize=12)\n",
    "ax[0].set_ylabel('Benefit (%)', fontsize=12)\n",
    "ax[1].set_xlim([0,7])\n",
    "ax[0].set_xlim([-0.1, 3.5])\n",
    "ax[0].legend(fontsize=12)\n",
    "ax[1].set_xticks([1,2,3,4,5,6,7])\n",
    "ax[0].set_xticks([0,0.5,1,1.5,2,2.5,3])\n",
    "ax[0].set_ylim([0,14])\n",
    "\n",
    "fig.subplots_adjust(wspace=0)  \n",
    "\n",
    "ax[0].text(-0.05, 0.2, 'a)', fontsize=16)\n",
    "ax[1].text(0.1, 0.2, 'b)', fontsize=16)\n",
    "\n",
    "plt.savefig('figures/impact_optimisation.png',dpi=300,facecolor='#ffffff')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 6: Comparison impact with known distribution\n",
    "\n",
    "The difference between the total removed particle mass if the initial distribution of particles is known and the total removed particle mass when using the SSIsink centrality. The difference is plotted as a function of how clean the coastline is initially (in %). For this calculation, a cleanup effort of 10% is applied and each calculation is repeated 1000 times with randomly distributed particle weight. Outliers are shown with diamond markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# test when cleaning at the 10% highest polluted locations vs. cleaning at 10% highest centrality - sensitivity to % polluted coastline\n",
    "\n",
    "centr_obj, centr, ranking = get_centrality_rankings(Tmatrix, grids, p_total, lon_ports, lat_ports)\n",
    "\n",
    "centr_names = ranking.columns.values.tolist()\n",
    "coastline = np.linspace(0,1,Tmatrix.shape[0]+1)*100\n",
    "\n",
    "iters = 30 # should be big enough to always reach steady state\n",
    "threshold = 0.95 # to find when in steady state\n",
    "\n",
    "coastline_fraction = 0.1 #index of where x% of coastline is cleaned\n",
    "clean_fraction = np.array([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9])*100\n",
    "polluted_fraction = np.array([1.0,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1])*100\n",
    "\n",
    "randomdistr = {}\n",
    "randomdistr['polluted']=[]\n",
    "randomdistr['advantage']=[]\n",
    "randomdistr['effective_removed']=[]\n",
    "randomdistr['optimal_removed']=[]\n",
    "\n",
    "name = centr_names[3]\n",
    "\n",
    "for i,c in enumerate(clean_fraction):\n",
    "    print(c)\n",
    "    for k in range(500):\n",
    "        initial_distribution=np.random.randint(1000, size=(Tmatrix.shape[0]+1))\n",
    "        where_is_clean = np.random.choice(np.arange(0,Tmatrix.shape[0]+1), int((Tmatrix.shape[0]+1)*c/100), replace=False)\n",
    "        initial_distribution[where_is_clean]=0\n",
    "        initial_distribution[-1]=0 #start with no plastic in the ocean\n",
    "\n",
    "        total_removed, optimal_removed = calculate_impact_distr(ranking[name], Tmatrix, name, iters, threshold, p_total, initial_distribution, int(coastline_fraction*len(coastline)))\n",
    "\n",
    "        randomdistr['polluted'].append(int(polluted_fraction[i]))\n",
    "        randomdistr['effective_removed'].append(total_removed)\n",
    "        randomdistr['optimal_removed'].append(optimal_removed)\n",
    "        randomdistr['advantage'].append(total_removed - optimal_removed)            \n",
    "        \n",
    "impacts = pd.DataFrame(randomdistr)\n",
    "impacts.advantage = impacts.advantage*100 # in %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# test using gaussian random field generator - sensitivity to correlation length scale\n",
    "\n",
    "#centr_obj, centr, ranking = get_centrality_rankings(Tmatrix, grids, p_total, lon_ports, lat_ports)\n",
    "\n",
    "import pickle\n",
    "no_data=False #script takes ~20 min to run, outcome is saved as pickle in input file\n",
    "\n",
    "# generate mask for the coastal locations\n",
    "coastmask = np.copy(gridnumbermask)\n",
    "coastmask[coastmask>0]=1\n",
    "coastmask[coastmask==0]=np.nan\n",
    "\n",
    "# field size for initial distribution field\n",
    "x = np.arange(0,len(gridlon[0,:]))\n",
    "y = np.arange(0,len(gridlat[:,0]))\n",
    "\n",
    "centr_names = ranking.columns.values.tolist()\n",
    "coastline = np.linspace(0,1,Tmatrix.shape[0]+1)*100\n",
    "\n",
    "iters = 30 # should be big enough to always reach steady state\n",
    "threshold = 0.99 # to find when in steady state\n",
    "\n",
    "coastline_fraction = 0.1 #cleanup effort\n",
    "corr_length = np.array([1,2,3,4,5,6,7,8,9,10])*4\n",
    "\n",
    "randomdistr2 = {}\n",
    "randomdistr2['correlation_length_scale']=[]\n",
    "randomdistr2['advantage']=[]\n",
    "randomdistr2['effective_removed']=[]\n",
    "randomdistr2['optimal_removed']=[]\n",
    "\n",
    "name = centr_names[3] # which centrality do we look at\n",
    "\n",
    "if no_data:\n",
    "    for i,Lcorr in enumerate(corr_length):\n",
    "        print(Lcorr)\n",
    "        model = gs.Gaussian(dim=2, var=1, len_scale=Lcorr/4)\n",
    "\n",
    "        for k in range(500):              \n",
    "            # create random initial distribution        \n",
    "            srf = gs.SRF(model)\n",
    "            random_field = srf.structured([x, y])\n",
    "            field = (random_field+np.abs(np.min(random_field)))*coastmask\n",
    "            initial_distribution = np.reshape(field[field>-1],-1)\n",
    "            initial_distribution = initial_distribution[grids.Grid_Number-1]\n",
    "            initial_distribution = np.append(initial_distribution,[0]) #start with no plastic in the ocean\n",
    "\n",
    "            total_removed, optimal_removed = calculate_impact_distr(ranking[name], Tmatrix, name, iters, threshold, p_total, initial_distribution, int(coastline_fraction*len(coastline)))\n",
    "\n",
    "            randomdistr2['correlation_length_scale'].append(Lcorr)\n",
    "            randomdistr2['effective_removed'].append(total_removed)\n",
    "            randomdistr2['optimal_removed'].append(optimal_removed)\n",
    "            randomdistr2['advantage'].append(total_removed-optimal_removed)            \n",
    "        \n",
    "    impacts2 = pd.DataFrame(randomdistr2)\n",
    "    impacts2.to_pickle('input/sensitivity_initial_distribution.pkl')\n",
    "else:\n",
    "    impacts2 = pd.read_pickle('input/sensitivity_initial_distribution.pkl')\n",
    "\n",
    "impacts2.advantage = impacts2.advantage*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# FIGURE optimal vs. effective removal \n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "########### random distribution\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,7))\n",
    "\n",
    "flierprops = dict(markerfacecolor='0.75', marker='d', markersize=3, linestyle='none')\n",
    "\n",
    "sns.boxplot(x='polluted', y='advantage',data=impacts, ax=ax[0], color='blue', \n",
    "            saturation=0.5, flierprops=flierprops,\n",
    "            width=0.4)\n",
    "xvals = np.unique(impacts.polluted)\n",
    "positions = range(len(xvals))\n",
    "ax[0].set_ylabel('Advantage (%)',fontsize=12)\n",
    "ax[0].set_xlabel('Fraction of coastline that is initially polluted (%)',fontsize=12)\n",
    "for patch in ax[0].artists:\n",
    "    r, g, b, a = patch.get_facecolor()\n",
    "    patch.set_facecolor((r, g, b, .3))\n",
    "ax[0].set_ylim([-100,10])\n",
    "\n",
    "means = [np.median(impacts[impacts.polluted == xi].advantage) for xi in xvals]\n",
    "ax[0].plot(positions, means, '--k', lw=2)\n",
    "\n",
    "############ length scale\n",
    "\n",
    "flierprops = dict(markerfacecolor='0.75', marker='d', markersize=3, linestyle='none')\n",
    "\n",
    "sns.boxplot(x='correlation_length_scale', y='advantage',data=impacts2, ax=ax[1], color='blue', \n",
    "            saturation=0.5, flierprops=flierprops,\n",
    "            width=0.4)\n",
    "xvals = np.unique(impacts2.correlation_length_scale)\n",
    "positions = range(len(xvals))\n",
    "ax[1].set_ylabel('Advantage (%)',fontsize=12)\n",
    "ax[1].set_xlabel('Correlation length scale of initial random distribution (km)',fontsize=12)\n",
    "for patch in ax[1].artists:\n",
    "    r, g, b, a = patch.get_facecolor()\n",
    "    patch.set_facecolor((r, g, b, .3))\n",
    "ax[1].set_ylim([-100,10])\n",
    "\n",
    "means = [np.median(impacts2[impacts2.correlation_length_scale == xi].advantage) for xi in xvals]\n",
    "ax[1].plot(positions, means, '--k', lw=2)\n",
    "\n",
    "############ SAVE\n",
    "\n",
    "ax[0].plot([-1,10],[0,0],'k',linewidth=0.3)\n",
    "ax[1].plot([-1,10],[0,0],'k',linewidth=0.3)\n",
    "ax[0].set_xlim([-0.5,9.5])\n",
    "ax[1].set_xlim([-0.5,9.5])\n",
    "\n",
    "ax[0].text(-0.4, -98, 'a)', fontsize=16)\n",
    "ax[1].text(-0.4, -98, 'b)', fontsize=16)\n",
    "\n",
    "plt.savefig('figures/comparison_advantages.png',dpi=300,facecolor='#ffffff')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fig 7: Location of optimal removal \n",
    "\n",
    "The 10% highest ranked coastline nodes by the centralities discussed in section 2.4. The coastline locations determined by the Retention Rate (a), the SSIsink centrality (d), the betweenness centrality (h) and the PRin centrality (i) are recommended cleanup locations for the Galapagos Islands management that potentially have high impact (section 3.2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# FIGURE - Location of 10% effort\n",
    "\n",
    "c10=int(0.1*len(coastline))\n",
    "alphas = np.linspace(0.2,1,c10)\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "\n",
    "minconn = 0\n",
    "new_matrix = Tmatrix.copy()\n",
    "new_matrix[new_matrix<minconn]=0\n",
    "centr_obj, centr, ranking = get_centrality_rankings(new_matrix, grids, p_total, lon_ports, lat_ports)    \n",
    "centr_names = ranking.columns.values.tolist()\n",
    "\n",
    "cmap = plt.cm.get_cmap('tab10')\n",
    "colors = np.linspace(0,1,len(centr_names))\n",
    "\n",
    "def get_indices(centrality_ranked, centr_name):\n",
    "    removal_order = np.array(centrality_ranked,dtype='int')-1\n",
    "    removal_order = np.argsort(removal_order)    \n",
    "    return removal_order\n",
    "\n",
    "fig = plt.figure(figsize=(8,11))\n",
    "gs = fig.add_gridspec(4,3, width_ratios=[1,1,1], wspace=0, hspace=0)\n",
    "extent = [-92, -89, -1.75, 1]\n",
    "shp = shapereader.Reader('input/GSHHS_f_L1_Galapagos.shp')\n",
    "\n",
    "# plot locations\n",
    "interest_names = [0,1,2,3,4,5,6,7,8,9]\n",
    "labels = ['a)','b)','c)','d)','e)','f)','g)','h)','i)','j)']\n",
    "teller=0\n",
    "for i in range(4):\n",
    "    for j in range(3):\n",
    "        if teller <= 9:\n",
    "            name = centr_names[interest_names[teller]]\n",
    "            print(name)\n",
    "\n",
    "            ax = fig.add_subplot(gs[i,j], projection=ccrs.PlateCarree()) \n",
    "            ax.set_extent(extent)\n",
    "            for record, geometry in zip(shp.records(), shp.geometries()):\n",
    "                ax.add_geometries([geometry], ccrs.PlateCarree(), facecolor=(216/255,216/255,216/255),\n",
    "                                  edgecolor='black')\n",
    "\n",
    "            removal_order = get_indices(ranking[name], name)\n",
    "            ax.scatter(grids.lon[removal_order[:c10]],grids.lat[removal_order[:c10]],\n",
    "                       s=29,\n",
    "                       color=cmap(colors[interest_names[teller]]),\n",
    "                       alpha=alphas,zorder=20)    \n",
    "            ax.scatter(-1,-1,\n",
    "                       s=29,\n",
    "                       color=cmap(colors[interest_names[teller]]),\n",
    "                       label=labels[teller]+' '+name)\n",
    "            ax.legend(loc='upper right')\n",
    "            teller+=1\n",
    "        \n",
    "plt.savefig('figures/impact_location.png',dpi=300,facecolor='#ffffff')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
